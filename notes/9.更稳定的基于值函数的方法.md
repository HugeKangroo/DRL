#  更稳定的基于值函数的方法

[TOC]

## DQN：让强化学习更像监督学习

### 值函数的强化学习的一些常见问题

**现有的优化方法依赖于独立同分布假设和静态优化目标假设。**

监督学习的数据符合上述假设。

与之相反，强化学习：

- 不能遵守独立同分布(IID)假设
- 不能有个静态(不变)的优化目标

#### 监督学习 vs. 强化学习

##### 监督学习

- **提前（advance)**获取到了一个数据集，将数据集预处理，**打乱**分离成小的数据集来训练
- **打乱**数据使得数据：
  - 避免发展出过拟合的偏差(bias)
  - 减少训练过程中的方差(variance)
  - 加速收敛(convergence)
  - 整体上，学习一个对**潜在数据生成过程**的更通用表达。
- 训练过程中，数据对应的目标值是**固定的。**

##### 强化学习

1. 违背独立同分布

- 相比于监督学习，数据是**在线(online)**收集的。导致了第t+1时刻生成的经验样本和第t时刻生成的经验样本是**相关的**。
- 当策略提升的时候，**策略的提升改变了潜在的数据生成过程**。这意味这新的数据是局部相关且不均匀分布的。（**目标改变导致数据的采样分布变化**）

2. 违背静态优化目标

- 目标会随着网络的更新发生**改变。**基于值函数的方法中，优化价值函数。因此价值函数的优化会影响价值函数的形状，进而影响目标的值。这意味这使用的目标至少是无效或者有偏差的。(**目标改变导致函数的值发生改变**)

##### 在NFQ中

- 通过使用batch训练
- 使用一个**固定的小数据集**迭代多次

1. 采集一个小数据集。
2. 计算目标。
3. 在下一次采样前，多次优化网络。

将上述过程放在一个大批量的采样数据中，网络更新是由遍布网络函数的多个点构成。这使得目标的变化**更稳定一些**。

### 使用目标网络

- 使用一个单独的目标网络。该网络在多个时间步中保持不变，计算一个**更稳定的目标值。**
- 通过目标网路来**保持目标值不变**，将问题从”chasing your own tail“转化成”人工将问题分解成多个小的监督问题，并将问题有序的展现给智能体。“
- 当固定目标网络时，目标网络会被保持多个时间步长。
- **这个方法提高了拟合的几率。**
- 但是并没有达到最优值，因为这在非线性函数逼近中是不存在的。然而，非线性函数往往能在**全局上收敛**。
- 该方法实质上减少了结果离散的机会。
- 能**更容易**的**区分相关状态**

#### 公式对比

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta_i)-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

公式（2）中$\max_{a}Q(s',a';\theta^{-})$表示一个一段时间之前网络的实例。这个实例被“冻结”了多个时间步长。这让梯度更新有时间去优化目标网络。使网路的学习更稳定。

#### 注意

- **目标网路**和**在线网络**是**网络权重的两个实例**:
  - 它们有着相同的网络结构。
  - 周期(按频率)更新目标网络，使目标网络能“匹配上”在线网络的权重。
  - 在线网络随着每个时间步更新。
- 目标网络的更新频率是根据实际问题决定的。
  - 10~1000个time step比较常见
  - 卷据图像网络 > 10,000 time steps比较常见
  - 简单直接的问题，例如cart-pole，10~20 time steps
- 虽然目标网络稳定了训练的目标值，但是**减慢了训练速度**

### 使用更大的网络

- **某种程度上**，另一种解决非静态问题的方法是使用更大的网络。
- 使用更大的网络更有可能检测出输入之间的**细微差别**
  - 更大的网络能够减少状态-动作对之间的混叠
    - 这里的混叠指的是，当两个状态**可以**看起来相同或者相似，但**实际**需要不同的动作。
    - 状态的混叠会发生在网络缺乏表达能力的时候。毕竟神经网络试图找到相似性来归纳(find similarities to generalize)。神经网络的目标是找到这些相似性。
    - 然而神经网络**过小**，会导致归纳出错。神经网络会倾向于简单，容易发现的规范。
  - 网络能力越强则混叠越少，混叠越少连续样本之间的相关性就不太明显。
  - 所以目标值和当前估计值之间看起来越独立。
- **但是**，需要**更长**的时间训练。
  - 更多的数据(需要更长的交互时间)
  - 更长的计算(处理)时间
- 相比更大的网络，使用目标网络是一种**更具鲁棒性**的方法。

### 目标网络 vs. 更大的网络

- 目标网络方法更具有鲁棒性。**创建了一个临时的静态目标。**

- 两种属性对最终性能有着相似的影响。**可以“看出”相似状态(比如，那些暂时相关的状态)下的不同之处。**

  - 网络的尺寸																											

  - 目标网络和网络的更新频率

- 目标网络方式被**多次证明是有效的**

- 大网络方法**不是永远奏效**

### 使用经验回放

- 由一个数据结构组成。这个数据结构通常被称为回放缓存(replay buffer or replay memory)

- 经验回放内会存放多个时间步长的数据样本(历史经验)。

- 经验回放允许从大量过去的经验中采样 mini-batches
  - 智能体的训练可以使用**更加**不同的mini-bathes(从回放中采样)来更新网络
  - 不需要对一个mini-bathes反复迭代多次(相比于NFQ而言)
  - 充分采样一个足够大的回放缓存，会使目标值智能**缓慢**的优化。所以通过回放机制每一个时间步下都可以采样和训练。并且结果偏离的风险很小。
  
- 经验回放的优点：
  - 通过**随机**从回放中采样，提高了低方差优化网络的概率(网络更新更稳定)
    - NFQ中，多数样本是**相关且相似**的。这样的样本使得**函数的变化集中在有限的区域。** 这潜在的**过分强调(overemphasize)**了更新的幅度(不稳定)。
    - 相反如果均匀地随机从回放中采样，由于样本很大可能是分布在各个区域，使得更新也分布在各个区域。能**更好**的表现**真实的价值函数**。
  - 经验回放使数据**看起来**更符合独立同分布，所以优化方法会更稳定。因为一个mini-batch的数据是一次性从多个轨迹，甚至策略中采样得到的。
  - 通常，一个回放缓存的容量在10,000~1,000,000个经验。取决于问题。一旦超过最大值时在插入新经验之前，将会释放**最老的经验**。
  - **但是，**高维的观测数据(例如图像或者多张图像)需要更大的缓存空间，进而可能很快达到**硬件**缓存空间的**限制**
  
  $$
  \nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
  $$
  
  $$
  \nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s' \sim \mathcal u(\mathcal D)}[(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
  $$
  
  注：
  
  1. $\mathcal u$指均匀采样，$\mathcal{D}$指回放缓存。**采样的方法可以由多种分布可选。**
  2. 公式(4)为整个DQN网络更新梯度。这被称为nature DQN(目标网络+经验回放)

### 使用其他的经验探索策略

- NFQ中使用了$\epsilon-greedy$探索策略：
  - **均匀**随机从0~1生成一个数number
  - 如果number < $\epsilon$, 则用**均匀分布**随机生成一个动作(这个随机动作可能包含贪婪推测出来的动作)
  - 如果number > $\epsilon$, 则使用贪婪动作。

```python
  class EGreedyStrategy():
      def __init__(self, epsilon=0.1):
          self.epsilon = epsilon
          self.exploratory_action_taken = None
  
      def select_action(self, model, state):
          self.exploratory_action_taken = False
          with torch.no_grad():
              q_values = model(state).cpu().detach().data.numpy().squeeze()
  
          if np.random.rand() > self.epsilon:
              action = np.argmax(q_values)
          else: 
              action = np.random.randint(len(q_values))
  
          self.exploratory_action_taken = action != np.argmax(q_values)
          return action
```

- 线性$\epsilon-greedy$探索策略

```python
class EGreedyLinearStrategy():
    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):
        self.t = 0
        self.epsilon = init_epsilon
        self.init_epsilon = init_epsilon
        self.min_epsilon = min_epsilon
        self.decay_steps = decay_steps
        self.exploratory_action_taken = None
        
    def _epsilon_update(self):
        epsilon = 1 - self.t / self.decay_steps
        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon
        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)
        self.t += 1
        return epsilon

    def select_action(self, model, state):
        self.exploratory_action_taken = False
        with torch.no_grad():
            q_values = model(state).cpu().detach().data.numpy().squeeze()

        if np.random.rand() > self.epsilon:
            action = np.argmax(q_values)
        else: 
            action = np.random.randint(len(q_values))

        self.epsilon = self._epsilon_update()
        self.exploratory_action_taken = action != np.argmax(q_values)
        return action
```



- 指数$\epsilon-greedy$探索策略

```python
class EGreedyExpStrategy():
    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):
        self.epsilon = init_epsilon
        self.init_epsilon = init_epsilon
        self.decay_steps = decay_steps
        self.min_epsilon = min_epsilon
        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01
        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon
        self.t = 0
        self.exploratory_action_taken = None

    def _epsilon_update(self):
        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]
        self.t += 1
        return self.epsilon

    def select_action(self, model, state):
        self.exploratory_action_taken = False
        with torch.no_grad():
            q_values = model(state).detach().cpu().data.numpy().squeeze()

        if np.random.rand() > self.epsilon:
            action = np.argmax(q_values)
        else:
            action = np.random.randint(len(q_values))

        self._epsilon_update()
        self.exploratory_action_taken = action != np.argmax(q_values)
        return action
```

- soft-max探索策略

```python
class SoftMaxStrategy():
    def __init__(self, 
                 init_temp=1.0, 
                 min_temp=0.3, 
                 exploration_ratio=0.8, 
                 max_steps=25000):
        self.t = 0
        self.init_temp = init_temp
        self.exploration_ratio = exploration_ratio
        self.min_temp = min_temp
        self.max_steps = max_steps
        self.exploratory_action_taken = None
        
    def _update_temp(self):
        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)
        temp = (self.init_temp - self.min_temp) * temp + self.min_temp
        temp = np.clip(temp, self.min_temp, self.init_temp)
        self.t += 1
        return temp

    def select_action(self, model, state):
        self.exploratory_action_taken = False
        temp = self._update_temp()

        with torch.no_grad():
            q_values = model(state).cpu().detach().data.numpy().squeeze()
            scaled_qs = q_values/temp
            norm_qs = scaled_qs - scaled_qs.max()            
            e = np.exp(norm_qs)
            probs = e / np.sum(e)
            assert np.isclose(probs.sum(), 1.0)

        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]
        self.exploratory_action_taken = action != np.argmax(q_values)
        return action
```



### DQN主要模块

> - 近似动作值函数$Q(s,a;\theta)$
> - 使用state-in-**values**-out 结构(输出节点的数量对应动作数量，表示每个state-action的值)方便比较动作值的大小（如果使用state-in-**value**-out 结构则不好比较不同动作的值）
> - 向着最优动作值函数$q^*(s,a)$优化动作值函数。
> - 使用离线策略的TD targets(Q-learning)去估计策略值。$r + \gamma * \max \limits_{a'} Q(s',a';\theta)$
> - 使用均方差(mse)作为损失函数。
> - 使用RMSprop左右优化器，学习速率为0.0005
> - 使用exponentially decaying epsilon-greedy来提高策略。通多大约20,000步，epsilon从1.0衰减到0.3。
> - 每15步更新一次目标网络
>
> 主要环节：
>
> 1. 采集经验元组$(s_t,A_t,R_{t+1},S_{t+1},D_{t+1})$ 并将经验插入到回放缓存中。
> 2. 随机从缓存中通过**均匀分布采样**一个mini-batch，为所有mini-batch内的经验计算离线 TD targets(Q-learning)
> 3. 利用MSE和RMSProp拟合动作值函数。

## Double DQN: 缓解高估动作值函数

