#  更稳定的基于值函数的方法

[TOC]

## DQN：让强化学习更像监督学习

### 值函数的强化学习的一些常见问题

**现有的优化方法依赖于独立同分布假设和静态优化目标假设。**

监督学习的数据符合上述假设。

与之相反，强化学习：

- 不能遵守独立同分布(IID)假设
- 不能有个静态(不变)的优化目标

#### 监督学习 vs. 强化学习

##### 监督学习

- **提前（advance)**获取到了一个数据集，将数据集预处理，**打乱**分离成小的数据集来训练
- **打乱**数据使得数据：
  - 避免发展出过拟合的偏差(bias)
  - 减少训练过程中的方差(variance)
  - 加速收敛(convergence)
  - 整体上，学习一个对**潜在数据生成过程**的更通用表达。
- 训练过程中，数据对应的目标值是**固定的。**

##### 强化学习

1. 违背独立同分布

- 相比于监督学习，数据是**在线(online)**收集的。导致了第t+1时刻生成的经验样本和第t时刻生成的经验样本是**相关的**。
- 当策略提升的时候，**策略的提升改变了潜在的数据生成过程**。这意味这新的数据是局部相关且不均匀分布的。（**目标改变导致数据的采样分布变化**）

2. 违背静态优化目标

- 目标会随着网络的更新发生**改变。**基于值函数的方法中，优化价值函数。因此价值函数的优化会影响价值函数的形状，进而影响目标的值。这意味这使用的目标至少是无效或者有偏差的。(**目标改变导致函数的值发生改变**)

##### 在NFQ中

- 通过使用batch训练
- 使用一个**固定的小数据集**迭代多次

1. 采集一个小数据集。
2. 计算目标。
3. 在下一次采样前，多次优化网络。

将上述过程放在一个大批量的采样数据中，网络更新是由遍布网络函数的多个点构成。这使得目标的变化**更稳定一些**。

### 使用目标网络

- 使用一个单独的目标网络。该网络在多个时间步中保持不变，计算一个**更稳定的目标值。**
- 通过目标网路来**保持目标值不变**，将问题从”chasing your own tail“转化成”人工将问题分解成多个小的监督问题，并将问题有序的展现给智能体。“

- 当固定目标网络时，目标网络会被保持多个时间步长。
- **这个方法提高了拟合的几率。**
- 但是并没有达到最优值，因为这在非线性函数逼近中是不存在的。然而，非线性函数往往能在**全局上收敛**。
- 该方法实质上减少了结果离散的机会。

#### 公式对比

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta_i)-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

### 使用更大的网络

### 使用经验回放

### 使用其他的经验探索策略