#  更稳定的基于值函数的方法

[TOC]

## DQN：让强化学习更像监督学习

### 值函数的强化学习的一些常见问题

**现有的优化方法依赖于独立同分布假设和静态优化目标假设。**

监督学习的数据符合上述假设。

与之相反，强化学习：

- 不能遵守独立同分布(IID)假设
- 不能有个静态(不变)的优化目标

#### 监督学习 vs. 强化学习

##### 监督学习

- **提前（advance)**获取到了一个数据集，将数据集预处理，**打乱**分离成小的数据集来训练
- **打乱**数据使得数据：
  - 避免发展出过拟合的偏差(bias)
  - 减少训练过程中的方差(variance)
  - 加速收敛(convergence)
  - 整体上，学习一个对**潜在数据生成过程**的更通用表达。
- 训练过程中，数据对应的目标值是**固定的。**

##### 强化学习

1. 违背独立同分布

- 相比于监督学习，数据是**在线(online)**收集的。导致了第t+1时刻生成的经验样本和第t时刻生成的经验样本是**相关的**。
- 当策略提升的时候，**策略的提升改变了潜在的数据生成过程**。这意味这新的数据是局部相关且不均匀分布的。（**目标改变导致数据的采样分布变化**）

2. 违背静态优化目标

- 目标会随着网络的更新发生**改变。**基于值函数的方法中，优化价值函数。因此价值函数的优化会影响价值函数的形状，进而影响目标的值。这意味这使用的目标至少是无效或者有偏差的。(**目标改变导致函数的值发生改变**)

##### 在NFQ中

- 通过使用batch训练
- 使用一个**固定的小数据集**迭代多次

1. 采集一个小数据集。
2. 计算目标。
3. 在下一次采样前，多次优化网络。

将上述过程放在一个大批量的采样数据中，网络更新是由遍布网络函数的多个点构成。这使得目标的变化**更稳定一些**。

### 使用目标网络

- 使用一个单独的目标网络。该网络在多个时间步中保持不变，计算一个**更稳定的目标值。**
- 通过目标网路来**保持目标值不变**，将问题从”chasing your own tail“转化成”人工将问题分解成多个小的监督问题，并将问题有序的展现给智能体。“
- 当固定目标网络时，目标网络会被保持多个时间步长。
- **这个方法提高了拟合的几率。**
- 但是并没有达到最优值，因为这在非线性函数逼近中是不存在的。然而，非线性函数往往能在**全局上收敛**。
- 该方法实质上减少了结果离散的机会。
- 能**更容易**的**区分相关状态**

#### 公式对比

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta_i)-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s'}[(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$

公式（2）中$\max_{a}Q(s',a';\theta^{-})$表示一个一段时间之前网络的实例。这个实例被“冻结”了多个时间步长。这让梯度更新有时间去优化目标网络。使网路的学习更稳定。

#### 注意

- **目标网路**和**在线网络**是**网络权重的两个实例**:
  - 它们有着相同的网络结构。
  - 周期(按频率)更新目标网络，使目标网络能“匹配上”在线网络的权重。
  - 在线网络随着每个时间步更新。
- 目标网络的更新频率是根据实际问题决定的。
  - 10~1000个time step比较常见
  - 卷据图像网络 > 10,000 time steps比较常见
  - 简单直接的问题，例如cart-pole，10~20 time steps
- 虽然目标网络稳定了训练的目标值，但是**减慢了训练速度**

### 使用更大的网络

- **某种程度上**，另一种解决非静态问题的方法是使用更大的网络。
- 使用更大的网络更有可能检测出输入之间的**细微差别**
  - 更大的网络能够减少状态-动作对之间的混叠
    - 这里的混叠指的是，当两个状态**可以**看起来相同或者相似，但**实际**需要不同的动作。
    - 状态的混叠会发生在网络缺乏表达能力的时候。毕竟神经网络试图找到相似性来归纳(find similarities to generalize)。神经网络的目标是找到这些相似性。
    - 然而神经网络**过小**，会导致归纳出错。神经网络会倾向于简单，容易发现的规范。
  - 网络能力越强则混叠越少，混叠越少连续样本之间的相关性就不太明显。
  - 所以目标值和当前估计值之间看起来越独立。
- **但是**，需要**更长**的时间训练。
  - 更多的数据(需要更长的交互时间)
  - 更长的计算(处理)时间
- 相比更大的网络，使用目标网络是一种**更具鲁棒性**的方法。

### 目标网络 vs. 更大的网络

- 目标网络方法更具有鲁棒性。**创建了一个临时的静态目标。**

- 两种属性对最终性能有着相似的影响。**可以“看出”相似状态(比如，那些暂时相关的状态)下的不同之处。**

  - 网络的尺寸																											

  - 目标网络和网络的更新频率

- 目标网络方式被**多次证明是有效的**

- 大网络方法**不是永远奏效**

### 使用经验回放

- 由一个数据结构组成。这个数据结构通常被称为回放缓存(replay buffer or replay memory)
- 经验回放内，会存放多个时间步长的数据样本。
- 允许从回放中采样 mini-batches

### 使用其他的经验探索策略