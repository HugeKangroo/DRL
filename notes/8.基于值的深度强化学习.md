# 基于值的深度强化学习

[TOC]



## 对非深度学习方法章节总结

- 第2章，介绍了将问题表述成强化学习可以解决的马尔可夫决策过程

- 第3章，描述了解决MDP问题的策略迭代和值迭代算法，规划算法。

- 第4章，在没有MDP模型的条件下，解决单步MDP问题（依靠估计）。多臂赌博机处理估计回馈（evaluative feedback）

- 第5章，在序列和不确定环境下，如何估计策略的价值

- 第6章，在不确定的序列决策问题下，计算最优策略

- 第7章，学习了最大化利用经验，计算最右策略的方法

- 第5，6，7章处理了**同步序列(simultaneously sequential)**和**估计(evaluative)**的回馈问题,解决这种问题的强化学习方法通常被称为**表强化学习(tabular reinforcement learning)**

## 目前常见的强化学习方法类型

- 基于值(value-based)
- 基于策略(policy-based)
- 基于演员评论家(actor-critic)
- 基于模型(model-based)
- 无梯度方法(gradient-free / derivative-free)

## 深度强化学习智能体使用的回馈类型

|                                                     | sequential（vs. one-shot） | Evaluative(vs. supervised) | Sampled(vs. exhaustive) |
| --------------------------------------------------- | -------------------------- | -------------------------- | ----------------------- |
| Supervised learning                                 | $\times$                   | $\times$                   | $\checkmark$            |
| Planning (chapter3)                                 | $\checkmark$               | $\times$                   | $\times$                |
| Bandits (chapter4)                                  | $\times$                   | $\checkmark$               | $\times$                |
| Tabular reinforcement learning (chapters 5,6,7)     | $\checkmark$               | $\checkmark$               | $\times$                |
| Deep reinforcement learning (chapters 8,9,10,11,12) | $\checkmark$               | $\checkmark$               | $\checkmark$            |

### 回馈带有序列性

- 时序反馈中的信息延迟,延迟回馈导致解释反馈的来源变得棘手。例如棋类游戏可能只有到游戏的最后才能反应出之前行为的好坏。
- **时序反馈引发了时间信用分配问题**。也即需要确定奖励来自于哪个state，action或者state-action pair。
- **需要同时兼顾短期目标和长期收益**。

#### 相比于one-shot feedback

如监督学习或者多臂赌博机问题，其决策没有长期后果。决策和决策之间是独立的。

- 例如分类一个图像，不论分类是否正确都不会影响之后的表现。但在深度强化学习中，这种时序的依赖是存在的。
- 多臂赌博机，在执行一个动作后回合会**立即**结束。

### 回馈带有估计性

- 表强化学习(tabular reinforcement learning)和赌博机(bandits)都在处理带有估计的回馈。
- 带有估计的回馈的难点是，**回馈的好坏是相对的**。因为环境是不确定的。而环境不确定的原因是不能获取到环境的MDP，也即转移函数和奖励函数
- 需要平衡**探索新信息和利用已有知识**之间的关系。由于探索新信息，会导致错过对已有知识的利用机会。**容易累计错误**(accumulate regret)。 这都是由于不确定性导致的。
- **通过探索新信息，提升当前的信息。**

#### 相比于supervised feedback

- supervised feadback的模型会受到监督。在训练中，每个采样样本会被给与**正确的标签**，**如果模型计算错误，则正确的答案会在之后立即给与**。
- 由于有正确的答案，监督回馈比估计回馈更容易处理。
- 赌博机虽然不用处理序列回馈问题(其状态是单一的)，但是其需要从估计回馈中学习。需要平衡探索和利用的需求。
- 当回馈**既是估计的也是序列的**，问题会**更加困难**。因为算法必须同时平衡瞬时和长期目标，并且收集和利用信息。
- 表强化学习(tabular reinforcement learning)和DRL都能从时序序列(simultaneously sequential)和估计反馈中学习

### 回馈带有采样性

- 表强化学习和深度强化学习的区别在于处理问题的复杂度。
- 深度强化学习处理的问题往往不能**穷举**所有的反馈。智能体需要使用收集到的反馈进行归纳(generalize)，并在此基础上提出明智的决策。
- **并不是所有的问题都能穷举**,所以表强化学习有一定的局限性。
- 监督学习也是从采样的样本中学习。而表强化学习和多臂赌博机则不是。

#### 相比于穷举回馈

- 穷举意味着智能体要能获取到**所有**可能的样本
- 表强化学习和多臂赌博机只需要采集足够长时间的样本，为获取最优性能提供足够的信息。能采集到**穷举的回馈**也是表强化学习能收敛到最优的先提条件。（能用表格建模的环境往往符合这种假设）
- 当处理高维或者连续状态，动作空间时；或者当需要快速处理这些问题时，表格方法不能起作用。

## 强化学习函数逼近的介绍

deep learning $=$ "non linear function approximation"

函数毕竟主要为了在表强化学习的基础上，解决下面两种问题。

### 强化学习问题可能有高维的状态和动作空间

- 强化学习环境可能有着**高**或者**极高**的（状态空间或者动作空间）维度。这个维度单单指描述一个状态所需要的变量数量。例如一张彩色图像可以有$w*h*channel$个变量。
- 使用表强化学习，在实际的复杂问题中描述价值函数是不实际的。换句话说，表方法无法描述复杂度高的问题。

### 强化学习问题可能有着连续状态或动作空间

- 强化学习环境可能还有连续变量。以为着一个变量可以有无线多的值。例如身高1.7m~1.8m，区间内可以包含无数个值。
- **即便不是连续变量，一个变量也可能需要一个很高维度或者很大的数量来表示。例如一个像素可以取值0~255**
- 低维状态空间也可以是无穷大的状态空间。例如机器人(x，y，z)的连续值。
- **虽然可以利用离散化解决，但是效率往往不高**

### 使用函数逼近的优势

- 通过函数逼近，智能体可以更少的数据学习和利用范式(patterns),同时也许会更快。
- **表格(离散)方法**每个索引下的值是**独立的**，**函数逼近**可以展示数据间的**潜在关系**
- 