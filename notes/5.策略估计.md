

# 5. 策略估计

本章节的方法实际估计的是价值函数，可以是状态值函数也可以是动作值函数。

##  monte-carlo learning

状态价值函数：
$$
v_{\pi} = \mathbb{E}_{\pi}[G_{t:T}S_t = s]
$$

折扣回报：
$$
G_{t:T} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T
$$


一个轨迹（trajectory）被定义为：
$$
S_t,A_t,R_{t+1},\dots,R_T,S_T \sim \pi_{t:T}
$$
计算采样轨迹每个状态的折扣回报和：
$$
Trajecoty_{cur}(S_t) = Trajectory_{pre}(S_t) + G_{t:T}
$$
同时计算每个状态采样到的次数：
$$
N_cur(S_t) = N_{pre}(S_t) + 1
$$
计算经验均值：
$$
V_{cur}(S_t) = \frac {Trajectory_{cur}(S_t)} {N_{cur}(S_t)}
$$
且：
$$
N_{cur}(s) \to \infin, \space \space V(s) \to v_{\pi}(s)
$$


**增量表达式：**
$$
V_{cur}(S_t) = V_{pre}(S_t) + \frac{1}{N_t(S_t)}[G_{t:T} - V_{pre}(S_t)]
$$
使用时间依赖或者常量的学习速率：
$$
V_{cur}(S_t) = V_{pre}(S_t) + \alpha_{t}[G_{t:T} - V_{pre}(S_t)]
$$

**特点：方差大，但偏差小**

```python
def mc_prediction(pi, 
                  env, 
                  gamma=1.0,
                  init_alpha=0.5,
                  min_alpha=0.01,
                  alpha_decay_ratio=0.5,
                  n_episodes=500, 
                  max_steps=200,
                  first_visit=True):
    nS = env.observation_space.n
    discounts = np.logspace(0, 
                            max_steps, 
                            num=max_steps, 
                            base=gamma, 
                            endpoint=False)
    alphas = decay_schedule(init_alpha, 
                            min_alpha, 
                            alpha_decay_ratio, 
                            n_episodes)
    V = np.zeros(nS, dtype=np.float64)
    V_track = np.zeros((n_episodes, nS), dtype=np.float64)
    targets = {state:[] for state in range(nS)}

    for e in tqdm(range(n_episodes), leave=False):
        trajectory = generate_trajectory(pi, 
                                         env, 
                                         max_steps)
        visited = np.zeros(nS, dtype=np.bool)
        for t, (state, _, reward, _, _) in enumerate(trajectory):
            if visited[state] and first_visit:
                continue
            visited[state] = True

            n_steps = len(trajectory[t:])
            G = np.sum(discounts[:n_steps] * trajectory[t:, 2])
            targets[state].append(G)
            mc_error = G - V[state]
            V[state] = V[state] + alphas[e] * mc_error
        V_track[e] = V
    return V.copy(), V_track, targets
```



## temporal-difference learning

**特点：方差小，但偏差大**

```python
def td(pi, 
       env, 
       gamma=1.0,
       init_alpha=0.5,
       min_alpha=0.01,
       alpha_decay_ratio=0.5,
       n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS, dtype=np.float64)
    V_track = np.zeros((n_episodes, nS), dtype=np.float64)
    targets = {state:[] for state in range(nS)}
    alphas = decay_schedule(
        init_alpha, min_alpha,
        alpha_decay_ratio, n_episodes)
    for e in tqdm(range(n_episodes), leave=False):
        state, done = env.reset(), False
        while not done:
            action = pi(state)
            next_state, reward, done, _ = env.step(action)
            td_target = reward + gamma * V[next_state] * (not done)
            targets[state].append(td_target)
            td_error = td_target - V[state]
            V[state] = V[state] + alphas[e] * td_error
            state = next_state
        V_track[e] = V
    return V, V_track, targets
```



## N-step TD learning learning 

特点：结合mc和td的特点，可以调节偏差和方差。

```python
def ntd(pi, 
        env, 
        gamma=1.0,
        init_alpha=0.5,
        min_alpha=0.01,
        alpha_decay_ratio=0.5,
        n_step=3,
        n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS, dtype=np.float64)
    V_track = np.zeros((n_episodes, nS), dtype=np.float64)
    discounts = np.logspace(0, n_step+1, num=n_step+1, base=gamma, endpoint=False)
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    for e in tqdm(range(n_episodes), leave=False):
        state, done, path = env.reset(), False, []
        while not done or path is not None:
            path = path[1:]
            while not done and len(path) < n_step:
                action = pi(state)
                next_state, reward, done, _ = env.step(action)
                experience = (state, reward, next_state, done)
                path.append(experience)
                state = next_state
                if done:
                    break

            n = len(path)
            est_state = path[0][0]
            rewards = np.array(path)[:,1]
            partial_return = discounts[:n] * rewards
            bs_val = discounts[-1] * V[next_state] * (not done)
            ntd_target = np.sum(np.append(partial_return, bs_val))
            ntd_error = ntd_target - V[est_state]
            V[est_state] = V[est_state] + alphas[e] * ntd_error
            if len(path) == 1 and path[0][3]:
                path = None

        V_track[e] = V
    return V, V_track
```



## TD($\lambda$) learning 

### forward-view TD($\lambda$)

forward-view 和mc类似，都需要采样完一个完整的轨迹之后才能计算。

一条轨迹： 
$$
S_t,A_t,R_{t+1},S_{t+1},\dots,R_T,S_T \sim \pi_{t:T}
$$

n步加权回报公式：

$$
G_{t:T}^{\lambda} = (1-\lambda) \sum \limits_{n-1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1}G_{t:T}
$$

$$
从1到T-1步的加权回报 \to (1-\lambda) \sum \limits_{n-1}^{T-t-1} \lambda^{n-1} G_{t:t+n}
$$

$$
加权的最终回报 \to \lambda^{T-t-1}G_{t:T}
$$

估计状态值函数： 
$$
V_{T}(S_t) = V_{T-1}(S_t) + \alpha_t[G_{t:T}^{\lambda}-V_{T-1}(S_t)]
$$
部分展开：
$$
one \space step:G_{t:t+1} = R_{t+1} + \gamma V_t(S_{t+1}) \to （1-\lambda）
$$

$$
two \space step:G_{t:t+2} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1}(S_{t+2}) \to (1-\lambda) \lambda
$$

$$
three \space step:G_{t:t+2} = R_{t+1} + \gamma R_{t+2} +  \gamma^2 R_{t+3} +\gamma^3 V_{t+2}(S_{t+3}) \to (1-\lambda) \lambda^2
$$

$$
n \space step:G_{t:t+n} = R_{t+1} + \dots + \lambda^{n-1}R_{t+n} + \lambda^{n}V_{t+n-1}(S_{t+n}) \to (1-\lambda)\lambda^{T-t-1}
$$

$$
terminal \space step:G_{t:T} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1}R_T \to \gamma^{T-t-1}
$$

特点：相比于backward-view，方差小但偏差大。

### backward-view TD($\lambda$) or TD($\lambda$)

- 利用资格迹(eligibility trace)记录最近访问的状态。
- 对每一步，追踪有资格更新估值的状态。track the states that are eligible for an update on every step
- 不单单追踪资格的有无，同时还要计算资格的大小

$$
E_0 = 0
$$

$$
S_t,A_t,R_{t+1},S_{t+1} \sim \pi_{t:t+1}
$$

$$
E_{t}(S_t) = E_{t}(S_t) + 1
$$

$$
\delta_{t:t+1}^{TD} = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)
$$

$$
V_{t+1} = V_t + \alpha_t \delta_{t:t+1}^{TD}(S_t)E_t
$$

$$
E_{t+1} = E_{t} \gamma \lambda
$$

```python
def td_lambda(pi, 
              env, 
              gamma=1.0,
              init_alpha=0.5,
              min_alpha=0.01,
              alpha_decay_ratio=0.5,
              lambda_=0.3,
              n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS, dtype=np.float64)
    E = np.zeros(nS, dtype=np.float64)
    V_track = np.zeros((n_episodes, nS), dtype=np.float64)
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    for e in tqdm(range(n_episodes), leave=False):
        E.fill(0)
        state, done = env.reset(), False
        while not done:
            action = pi(state)
            next_state, reward, done, _ = env.step(action)
            td_target = reward + gamma * V[next_state] * (not done)
            td_error = td_target - V[state]
            E[state] = E[state] + 1
            V = V + alphas[e] * td_error * E
            E = gamma * lambda_ * E
            state = next_state
        V_track[e] = V
    return V, V_track
```

