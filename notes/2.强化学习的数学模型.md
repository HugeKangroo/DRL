## 智能体(agent)：决策者



## 环境(environment)：除智能体之外的部分

### 马尔可夫决策过程：环境的核心

#### 状态

- 状态空间$S$可以是有限或者无穷的

- **描述状态的变量**必须有限且固定相同大小维度，例如一张rgb彩色图$w*h*channel*3$，而变量的取值可以是任意类型和大小。甚至可以是混合类型。

- 不同的观测性

  - MDPS： 完全可观测，即$Obs(state) = state = obervation$ 
  - POMDPS: 部分可观测，即$Obs(state) = partial\space state = observation\neq state $ 

- 状态和状态之间是独立的，只依靠当前的状态指导接下来可能的状态。给定当前状态和一个动作，转移到下一动作的概率是**独立于**历史交互的。即：
  $$
  P(S_{t+1}|S_t,A_t) = P(S_{t+1}|S_t,A_t,S_{t-1},A_{t-1},\dots)
  $$

  - 由于强化学习和深度强化学习是利用MDP假设建模，所以需要**尽可能**保持**必要的变量**来维持MDP假设。
  - 现实中可以从数据中提取更高维度的信息。例如可以从速度推导加速度。实际输入的变量越少，则很大概率智能体获取的信息是不充足的。

- MDP中所有状态的集合$S^+$,初始状态集合$S^i$. $P(S^i) = constant$

- 由MDP定义的强化学习问题中，状态转移在训练中**必须**是一致的。**每个状态的转移概率是保持不变的**，不论是训练的开始回合还是终止回合或者智能体评估阶段。

#### 终止状态

- 终止状态$S$

- 可以为所有的终止转移设置一个终止状态。也可以为设置多个终止状态。

  ```mermaid
  graph TD;
  h1[hole1]
  h2[hole2]
  g1[goal1]
  g2[goal2]
  t1[terminal]
  ht1[hole1 terminal]
  ht2[hole2 terminal]
  gt1[goal1 terminal]
  gt2[goal2 terminal]
  subgraph singleTerminal
  
  h1 --"probability 1 and reward 0"--> t1
  h2 --"probability 1 and reward 0"--> t1
  g1 --"probability 1 and reward 0"--> t1
  g2 --"probability 1 and reward 0"--> t1
  end
  
  subgraph MultiTerminal
  ht1 --"probability 1 and reward 0"--> ht1
  ht2 --"probability 1 and reward 0"--> ht2
  gt1 --"probability 1 and reward 0"--> gt1
  gt2 --"probability 1 and reward 0"--> gt2
  end
  ```

  

- **终止状态是一个特殊的状态**。终止状态下可以获得有所的动作转移，但只有转移向终止状态的**概率为1**，并且**奖励为0**。也即终止状态不论什么动作都会转移到其自身，逻辑上是一个无休止的循环。**注意，这里指的是终止状态向终止状态的转移。** **而不是**从其他非终止状态转移到终止状态。

$$
P_{transition}(s_{teminal},a,s_{non_terminal}) = 0,\forall a \in A
$$

$$
P_{transition}(s_{teminal},a,s_{terminal}) = 1, \forall a \in A
$$

$$
R(s_{terminal},a) = 0
$$

- 虽然通常每个回合的结尾会提供一个非零的奖励，但一种更兼容的约定是为终止状态的所有转移**百分百**指向终止状态自身，并且奖励为0(一旦进入终止状态如论动作为何，都会回到终止状态。且奖励为0). 这么做使得**所有的算法能向着相同的解决方法收敛**，否则可能会有无穷循环，且算法可能完全不起作用的风险。
- **个人理解：**这里终止状态的描述是站在**环境的角度**来看的，也即环境知道任务的结束或者终止。而实际的**智能体的策略学习**中，策略需要对每一个状态进行规划，其中包含了最终状态。如果单纯的给最终状态一个终止，则终止状态会**形成一个突变**。相当于一个单纯的截断；而如果使最终状态在给定任意动作奖励都为0的死循环(**价值函数为0**）, 更符合实际的终止状态的转移方程和奖励函数逻辑。

#### 动作

#### 转移函数

#### 奖励信号

#### 视野

#### 折扣因子

#### 马尔可夫决策过程扩展

