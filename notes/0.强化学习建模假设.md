# 强化学习的建模假设

[TOC]

## on-policy RL

### 例如

Monte-Carlo control和SARSA

### 要求

在无线探索下的有限贪婪,Greedy in the limit with infinite exploration(GLIE)是在线策略强化学习算法**一定**要满足的**必要条件**

1. 所有的state-action对**必须**被经常且无限次(inifinte often)的探索到
2. 策略**必须**要能在一个贪婪策略下收敛

所以在实际中，**必须**要将epsilon向着0衰减。既不能太快，第一条未必来得及满足；也不能太慢，将会花费更多的时间去收敛。



## off-policy RL

### 要求

1. 所有的state-action对**必须**被经常且无限次(inifinte often)的探索到
2. ~~策略**必须**要能在一个贪婪策略下收敛~~

而第二条不需要遵守。因为在off-policy中学习的策略和交互(采样)的策略不同。



## stochastic approximation RL

随机近似逼近。从采样学习，并且其样本间有一定的变化。所以估计的模型不会收敛除非让学习速率向着0变化。

### 要求

1. 学习速率的**和(sum)**必须是无穷的。
2. 学习速率的**平方和(sum of squares)**必须是有限的

所以，必须选择一个学习速率**会衰减但是永远不会到0**。初始时较大的学习速率，可以使算法**不过于注重**单个样本；当衰减的比较小时，可以使算法找到噪音背后的信号(信息)

**现实通常情况下**，学习速率会根据实际问题被设置为一个足够小的常数。同时，**较小的常数**对于非静态环境有着**更好的效果**。而非静态环境在实际世界中更为常见，通用。