## 回报（return）
- 从时刻 $t$ 到最终时刻 $T$：
$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T$$

- 增加折扣因子（discount factor）,给与靠近当前状态的时刻优势：
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^3 R_{t+3} + \dots + \gamma^{T-1}R_T$$

- 递归表示上式：
  $$G_t = R_{t+1} + \gamma G_{t+1}$$
  $$\because G_t = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}$$


# 价值函数
## 状态值函数
状态值函数被定义为：**给定状态，按照当前策略可以获得**的累计回报的期望。公式为：
$$v_{\pi} = \mathbb{E}_{\pi}[G_{t}|S_t=s]$$
将$G_{t}$展开
$$v_{\pi} = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^3 R_{t+3} + \dots + \gamma ^{T-1}R_T|S_t=s]$$
使用递归表达式：
$$v_{\pi} = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}|S_t=s]$$
完全展开：
$$v_{\pi} = \sum_{a}\pi(a|s) \sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi}(s') ], \forall s \in S$$

1. $\pi(a|s)$表示了在给定$s$的情况下，策略给出动作$a$**概率**。
2. $p(s',r|s,a)$表示了给定状态$s$,执行动作$a$,转移和奖励函数得到$s'$和$r$的**概率**。
3. $p(s',r|s,a)[r + \gamma v_{\pi}(s') ]$表示了一个状态$s$，在动作$a$下经过转移函数和奖励函数，获得$s'$和$r$的累计期望。
4. 整个公式的可以理解为两个部分：
   1. 根据状态转移函数和奖励函数，在给定动作a和当前状态s下所有可能状态的累计期望和
    $$\sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi}(s')]$$
   2. 根据策略函数$\pi$，计算包含策略的情况下的整体期望。策略函数为上式提供了动作$a$和$a$发生的概率。
    $$ \sum_{a}\pi(a|s) \sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi}(s')] $$
   3. 当假设策略函数$\pi$为**决断（deterministic）策略**，则公式简化为：
   $$\sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi}(s')] $$

## 动作值函数
动作值函数被定义为：**给定状态和动作,之后坚持按照策略行动**的累计回报的期望。其表达了当前状态s下的采取动作a,**并且之后依旧坚持策略$\pi$**所带来的累计期望。

公式为：
$$q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s,A_t = a]$$
递归表达：
$$q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_t + \gamma G_{t+1}|S_t = s,A_t = a]$$
利用**贝尔曼等式**表达：
$$q_{\pi}(s,a) = \sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi}(s') ], \forall s \in S , \forall a \in A(s)$$

## 状态值函数和动作值函数的关系
$$v_{\pi}(s)=\sum \limits_{a \in A} \pi (a|s) * Q_{\pi}(a,s)$$

 当假设策略函数$\pi$为**决断（deterministic）策略**，则公式简化为：
$$v_{\pi}(s) = Q_{\pi}(a',s) $$ 因为当$\pi(a'|s) = 1$时对应的$a'$才使值有效。


## 动作优势函数
动作优势函数被定义为：**策略$\pi$下，状态$s$下状态$a$的动作值函数与状态$s$的状态值函数的差**，表达式为：
$$ a_{\pi}(s,a) = q_{\pi}(s,a) - v_{\pi}(s)$$

其意义可以被理解为：**当前的状态s下，采取动作a和当前状态s下,采取默认动作的差**

# 最优价值函数
## 最优状态值函数
定义：（最优策略下）一个状态值函数对应其他所有状态值函数，每个状态都获得最大状态值。
$$v_{*}(s) = \max \limits_{\pi} v_{\pi}(s)$$

利用动作值函数表达：
$$v_{*}(s) = \max \limits_{a} q_{*}(s,a) = \max \limits_{a} \sum \limits_{s',r}p(s',r|s,a)[r+\gamma v_{*}(s')]$$

### [证明](https://mathoverflow.net/questions/321701/proof-of-bellman-optimality-equation-for-finite-markov-decision-processes)

#### 反证：
$$\because v_{\pi} = \mathbb{E}_{a}[q_{\pi}(s,a)]$$
$$\therefore v_{\pi}(s) \leq \max \limits_{a \in A}q_{\pi}(s,a)  \forall \pi \in \Pi$$

#### 假设：
$$v_{\pi_*}(s) < \max_{a \in A}q_{\pi_*}(s,a) = q_{\pi_*}(s,a_s)$$
该公式说明甚至有一个更好的策略$\pi_{*}^{'}(a|s)$,在状态$s$一定选择$a_s$,使得$v_{\pi_{*}}(s) < v_{\pi_{*}^{'}}(s) = q_{\pi_{*}(s,a_s)}$,这违背了$\pi_{*}$是最优策略 

## 最优动作值函数
定义：（最优策略下）一个动作值函数对应其他所有动作值函数，每个状态-动作对应的动作值最大。
$$q_{*}(s,a) = \max \limits_{\pi}q_{\pi}(s,a), \forall s \in S, \forall a \in A(s)$$

$$q_{*}(s,a) = \sum \limits_{s^{'},r}p(s^{'},r|s,a)[r + \gamma \max \limits_{a^{'}}q_{*}(s^{'},a^{'})]$$


## 最优优势函数
~~最优优势函数的值在所有的"状态-动作"中**小于或者等于0**~~

最优优势函数的值在所有的"状态-动作"中**等于0**，这表示**没有动作比默认动作有优势**

也即默认行为(策略)已经到达最优