# Markov Decision Process(MDP)

$$
\mathcal{MDP(S,A,T,R,S_{\theta},\gamma,H)}
$$

$$
\mathcal{S} \to {state \space space}
$$

$$
\mathcal{A} \to {action\space space}
$$

$$
\mathcal{T} \to {transition\space function}
$$

$$
\mathcal{R} \to {reward\space signal}
$$

$$
\mathcal{S_{\theta}} \to {initial\space states\space distribution}
$$

$$
\mathcal{\gamma} \to {discount\space factor}
$$

$$
\mathcal{H} \to {planning\space horizon}
$$

# Partially Obervation Markov Decision Process(POMDP)
$$
\mathcal{POMDP(S,A,T,R,S_{\theta},\gamma,H,O,E)}
$$

$$
\mathcal{S} \to {state \space space}
$$

$$
\mathcal{A} \to {action\space space}
$$

$$
\mathcal{T} \to {transition\space function}
$$

$$
\mathcal{R} \to {reward\space signal}
$$

$$
\mathcal{S_{\theta}} \to {initial\space states\space distribution}
$$

$$
\mathcal{\gamma} \to {discount\space factor}
$$

$$
\mathcal{H} \to {planning\space horizon}
$$

$$
\mathcal{O} \to {obervation\space space}
$$

$$
\mathcal{E} \to {emission\space probability} \to \mathcal{E(o_t|s_t)}
$$

