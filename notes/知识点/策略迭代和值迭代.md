# 策略迭代和价值迭代

策略迭代（policy iteration）：

- 估计策略：输入一个策略，估计这个策略的价值函数（状态值函数）
- 提升策略：利用状态值函数去获得更好的策略。
- 一旦上面的迭代不再产生变化，则策略和状态值函数都是最优的
- 使策略完全贪婪（entirely greedy）,根据估计策略的状态值函数。**这仅在由MDP的情况下。**

值迭代（value iteration）：

- 估计策略：和策略迭代不同的是，在值迭代的策略估计中，对于策略的估计不需要准确，只进行一次估计迭代（one single iteration）。策略估计阶段不计算实际的状态值函数。这种方法使策略估计阶段朝着实际的状态值函数移动。即时在策略评估阶段截断了一部分评估过程，但和策略迭代一样这种方式同样能够计算出最优值函数。
- 提升策略：改变策略，使策略根据值函数更加“贪婪”

**在没有MDP的情况下，不能完全依赖贪婪算法。相比于完全贪婪，在没有MDP的环境下，需要使策略保持一定的贪婪性，同时为探索保留空间。**