#  off-policy 和 on-policy

- on-poliy: 在线策略。智能体在学习的策略和生成经验（generating experience）的策略**相同**。只从当前错误中学习。
- off-poliy: 离线策略。智能体在学习的策略和生成经验（generating experience）的策略**不同**。只从之前的错误中学习。
  1. 行为策略(behavior policy) : 和环境交互生成经验的策略。
  2. 目标策略(target policy ) : 算法学习的目标策略。

## 举例：SARSA和Q-learning 对比

- SARA(on-policy)

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha_t[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]
$$

- Q-learning(off-policy)

  

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha_t[R_{t+1} + \gamma \max\limits_a Q(S_{t+1},a) - Q(S_t,A_t)]
$$

1. 本书中，Q-learning的select_action 使用的是衰减ε贪婪策略，而学习的目标策略 a来自：$a \in \arg\max Q(s)$   
2. 本书中，SARA的select_action 使用的是衰减ε贪婪策略，而学习的目标策略为衰减ε贪婪策略，a来自select_action。

公式说明：

- $S_t,A_t$分别表示当前时刻下的状态和动作。
- $S_{t+1},A_{t+1}$分别表示下一时刻的状态和动作。

- $a \in A(s)$ 表示状态s下所有可能的动作a。例如，九宫格中间一格可以有上下左右四个动作。因此$\max \limits_a Q(S_{t+1},a)$表示下一状态不同动作a的动作值函数最大值。