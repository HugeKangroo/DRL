#  off-policy 和 on-policy

- on-poliy: 在线策略。智能体在学习的策略和生成经验（generating experience）的策略**相同**。只从当前错误中学习。

- off-poliy: 离线策略。智能体在学习的策略和生成经验（generating experience）的策略**不同**。只从之前的错误中学习。
  1. 行为策略(behavior policy) : 和环境交互生成经验的策略。
  2. 目标策略(target policy ) : 算法学习的目标策略。
  
- 两种策略相比：

  - on-policy不是直接学习最优策略，而是学习一个仍然可以探索的策略。

  > - 用要学习的策略产生数据，再用产生的数据更新策略。
  >
  > - 样本效率低，早期的数据会被抛弃，只有**最新的**策略和其对应产生的数据参与更新。

  - off-policy是学习两个策略，学习(目标)策略和行为(探索)策略。所以其实目标策略和学习中产生的数据(来自探索策略)是不相关的。

  > - 方差大，收敛慢，但更通用

## 举例：SARSA和Q-learning 对比

- SARA(on-policy)

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha_t[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]
$$

- Q-learning(off-policy)


$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha_t[R_{t+1} + \gamma \max\limits_a Q(S_{t+1},a) - Q(S_t,A_t)]
$$

1. 本书中，Q-learning的select_action 使用的是衰减ε贪婪策略，而学习的目标策略 a来自：$a \in \arg\max Q(s)$   
2. 本书中，SARA的select_action 使用的是衰减ε贪婪策略，而学习的目标策略为衰减ε贪婪策略，a来自select_action。

公式说明：

- $S_t,A_t$分别表示当前时刻下的状态和动作。
- $S_{t+1},A_{t+1}$分别表示下一时刻的状态和动作。

- $a \in A(s)$ 表示状态s下所有可能的动作a。例如，九宫格中间一格可以有上下左右四个动作。因此$\max \limits_a Q(S_{t+1},a)$表示下一状态不同动作a的动作值函数最大值。