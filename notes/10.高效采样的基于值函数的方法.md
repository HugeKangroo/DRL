# 高效采样的基于值函数的方法

[TOC]

## Dueling DDQN

Dueling实际是一种改变网络结构的方法而并非改变算法本身，可以是Dueling DQN,也可以是 Dueling DDQN。

### 强化学习不是监督学习问题

- 虽然之前的设计思路是将强化学习向着监督学习的方式转化，但实际强化学习和监督学习是不同的。
  - replay buffer $\to$ 打散数据，使数据更符合独立同分布(IID)。
  - target network $\to$ 维护目标值或者所谓“true value”的稳定性。
  - **但强化学问题的本身是强化学习问题，这样的转化不是最明智的思路**

- 通过**动作优势函数**取代强化学习转化成监督学习的转化思路。

### 基于值函数的深度强化学习方法的细微差别

- 动作值函数可以表示为状态值函数+动作优势函数

$$
a_\pi(s,a) = q_\pi(s,a) - v_\pi(s) \to q_\pi(s,a) =v_\pi(s) + a_\pi(s,a)
$$

- 所以可以将动作值函数**分解**为两个部分：
  - 一部分时不同actions共享的($v_\pi(s)$) | 不依靠具体的动作
  - 一部分是表示各个action之间独一无二的($a_\pi(s,a)$) | 依靠具体的动作a
- 通常的直接学习动作值函数的方法是**单独**学习**每个动作**，这样是**低效的**。
  - 虽然神经网络是**内部联系的**，使得每个动作的学习具有一定的**相关性**。但这依旧是**低效的**。这些内联信息只会在神经网络的内部节点之间共享。
  - 这种低效具体的讲是：当给定一个信息，其实只学习了$Q(s,a_1)$,而忽略了可以同时通过这个信息学习$Q(s,a_2),Q(s,a_3),\dots Q(s,a_n), n \in |A(s)|$ 
  - $A$指的是某状态下的动作空间函数 
- 在一个**“好”的状态**下，采用一个**“坏”的动作** ***好过*** 在一个**“坏”的状态**下，采用一个**“好”的动作**
- dueling 网络结构将$Q(s,a)$ 的依赖关系使用在$V(s)$上，所以**每一次更新都会提升**$V(s)$的估计。对$V(s)$的估计是s下所有动作公用的。

### 利用优势函数的好处

优势函数表示了：在当前动作s下采用特定动作a的价值比平均值(状态值)好多少

### 一种强化学习感知架构

- dueling 网络结构有两个分离的“评价器”。
  - 一个估计state-value函数
  - 一个估计action-advantage函数
- 两个”评价者“共享一些内部节点，比如共享的特征提取层和某些隐藏层。
- 输出层之前，将网络的输出分离成两个”评价器“
  - state-value 函数，单个节点$estimator_V(s)=value$
  - action-advantage 函数, 多个节点（输入状态下可以使用的动作相同数量的节点。$estimator_A(s) = values$）
- 最终将两个"评价器"合并，计算动作值函数。

### 建造一个dueling 网络

### 重建动作值函数

$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha)
$$

$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \lgroup A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \sum \limits_{a'}A(s,a';\theta,\alpha) \rgroup
$$

符号说明：

- $\theta$表示**共享层**的权重

- $\alpha$表示**动作优势函数**部分的权重

- $\beta$表示**状态值函数**部分的权重

- 由于公式(2)无法还原一个唯一的Q函数(含有多中可能的解)，所以使用公式(3)

  >But, how do we join the outputs? Some of you are thinking, add them up, right? I mean, that’s the definition that I provided, after all. Though, several of you may have noticed that there is no way to recover V(s) and A(s, a) uniquely given only Q(s, a). Think about it; if you add +10 to V(s) and remove it from A(s, a) you obtain the same Q(s, a) with two different values for V(s) and A(s, a)

- 公式(3)使得状态值函数和动作优势函数**失去了其原来的含义**。使得状态值函数和动作优势函数偏移了一个常量，但同时稳定了优化过程。这个偏移不会影响动作优势函数的相对秩序，所以动作值函数的秩序也能保证。

- **论文实际中**有两种方式：

  - max

  $$
  Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + (A(s,a;\theta,\alpha) - \max \limits_{a' \in |\mathcal{A}|}(A(s,a';\theta,\alpha)))
  $$

  

  - average
    $$
    Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \lgroup A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \sum \limits_{a'}A(s,a';\theta,\alpha) \rgroup
    $$

  - $\theta, \alpha,\beta$ 分别表示：共享层权重，状态函数权重，动作函数权重。

  - [两种方法的解释和说明](https://ai.stackexchange.com/questions/8128/difficulty-in-understanding-identifiability-in-the-dueling-network-architecture)

  - 公式(4)**没有改变**Q函数的实际意义，因为改动使得优势函数部分增加了**潜在**的函数约束。

  - 公式(5)**改变了**Q的实际意义，使得Q值一直有个求均值带来的常量。但优点是：公式(4)的改变速度由最大值决定，公式(5)的改变速度由均值决定。

  - 两种方法没有改变优势函数的相对秩序。