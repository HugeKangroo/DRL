# 高效采样的基于值函数的方法

[TOC]

## Dueling DDQN

Dueling实际是一种改变网络结构的方法而并非改变算法本身，可以是Dueling DQN,也可以是 Dueling DDQN。

### 强化学习不是监督学习问题

- 虽然之前的设计思路是将强化学习向着监督学习的方式转化，但实际强化学习和监督学习是不同的。
  - replay buffer $\to$ 打散数据，使数据更符合独立同分布(IID)。
  - target network $\to$ 维护目标值或者所谓“true value”的稳定性。
  - **但强化学问题的本身是强化学习问题，这样的转化不是最明智的思路**

- 通过**动作优势函数**取代强化学习转化成监督学习的转化思路。

### 基于值函数的深度强化学习方法的细微差别

- 动作值函数可以表示为状态值函数+动作优势函数

$$
a_\pi(s,a) = q_\pi(s,a) - v_\pi(s) \to q_\pi(s,a) =v_\pi(s) + a_\pi(s,a)
$$

- 所以可以将动作值函数**分解**为两个部分：
  - 一部分时不同actions共享的($v_\pi(s)$) | 不依靠具体的动作
  - 一部分是表示各个action之间独一无二的($a_\pi(s,a)$) | 依靠具体的动作a
- 通常的直接学习动作值函数的方法是**单独**学习**每个动作**，这样是**低效的**。
  - 虽然神经网络是**内部联系的**，使得每个动作的学习具有一定的**相关性**。但这依旧是**低效的**。这些内联信息只会在神经网络的内部节点之间共享。
  - 这种低效具体的讲是：当给定一个信息，其实只学习了$Q(s,a_1)$,而忽略了可以同时通过这个信息学习$Q(s,a_2),Q(s,a_3),\dots Q(s,a_n), n \in |A(s)|$ 
  - $A$指的是某状态下的动作空间函数 
- 在一个**“好”的状态**下，采用一个**“坏”的动作** ***好过*** 在一个**“坏”的状态**下，采用一个**“好”的动作**
- dueling 网络结构将$Q(s,a)$ 的依赖关系使用在$V(s)$上，所以**每一次更新都会提升**$V(s)$的估计。对$V(s)$的估计是s下所有动作公用的。

### 利用优势函数的好处

优势函数表示了：在当前动作s下采用特定动作a的价值比平均值(状态值)好多少

### 一种强化学习感知架构

- dueling 网络结构有两个分离的“评价器”。
  - 一个估计state-value函数
  - 一个估计action-advantage函数
- 两个”评价者“共享一些内部节点，比如共享的特征提取层和某些隐藏层。
- 输出层之前，将网络的输出分离成两个”评价器“
  - state-value 函数，单个节点$estimator_V(s)=value$
  - action-advantage 函数, 多个节点（输入状态下可以使用的动作相同数量的节点。$estimator_A(s) = values$）
- 最终将两个"评价器"合并，计算动作值函数。

### 建造一个dueling 网络

### 重建动作值函数

$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha)
$$

$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \lgroup A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \sum \limits_{a'}A(s,a';\theta,\alpha) \rgroup
$$

符号说明：

- $\theta$表示**共享层**的权重

- $\alpha$表示**动作优势函数**部分的权重

- $\beta$表示**状态值函数**部分的权重

- 由于公式(2)无法还原一个唯一的Q函数(含有多中可能的解)，所以使用公式(3)

  >But, how do we join the outputs? Some of you are thinking, add them up, right? I mean, that’s the definition that I provided, after all. Though, several of you may have noticed that there is no way to recover V(s) and A(s, a) uniquely given only Q(s, a). Think about it; if you add +10 to V(s) and remove it from A(s, a) you obtain the same Q(s, a) with two different values for V(s) and A(s, a)

- 公式(3)使得状态值函数和动作优势函数**失去了其原来的含义**。使得状态值函数和动作优势函数偏移了一个常量，但同时稳定了优化过程。这个偏移不会影响动作优势函数的相对秩序，所以动作值函数的秩序也能保证。

- **论文实际中**有两种方式：

  - max

  $$
  Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + (A(s,a;\theta,\alpha) - \max \limits_{a' \in |\mathcal{A}|}(A(s,a';\theta,\alpha)))
  $$

  

  - average
    $$
    Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \lgroup A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \sum \limits_{a'}A(s,a';\theta,\alpha) \rgroup
    $$

  - $\theta, \alpha,\beta$ 分别表示：共享层权重，状态函数权重，动作函数权重。

  - [两种方法的解释和说明](https://ai.stackexchange.com/questions/8128/difficulty-in-understanding-identifiability-in-the-dueling-network-architecture)

  - 公式(4)**没有改变**Q函数的实际意义，因为改动使得优势函数部分增加了**潜在**的函数约束。

  - 公式(5)**改变了**Q的实际意义，使得Q值一直有个求均值带来的常量。但优点是：公式(4)的改变速度由最大值决定，公式(5)的改变速度由均值决定。

  - 两种方法没有改变优势函数的相对秩序。

  ```python
  class FCDuelingQ(nn.Module):
      def __init__(self, 
                   input_dim, 
                   output_dim, 
                   hidden_dims=(32,32), 
                   activation_fc=F.relu):
          super(FCDuelingQ, self).__init__()
          self.activation_fc = activation_fc
  
          self.input_layer = nn.Linear(input_dim, hidden_dims[0])
          self.hidden_layers = nn.ModuleList()
          for i in range(len(hidden_dims)-1):
              hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])
              self.hidden_layers.append(hidden_layer)
          self.output_value = nn.Linear(hidden_dims[-1], 1)
          self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
  
          device = "cpu"
          if torch.cuda.is_available():
              device = "cuda:0"
          self.device = torch.device(device)
          self.to(self.device)
          
      def _format(self, state):
          x = state
          if not isinstance(x, torch.Tensor):
              x = torch.tensor(x, 
                               device=self.device, 
                               dtype=torch.float32)
              x = x.unsqueeze(0)      
          return x
  
      def forward(self, state):
          x = self._format(state)
          x = self.activation_fc(self.input_layer(x))
          for hidden_layer in self.hidden_layers:
              x = self.activation_fc(hidden_layer(x))
          a = self.output_layer(x)
          v = self.output_value(x).expand_as(a)
          q = v + a - a.mean(1, keepdim=True).expand_as(a)
          return q
  
      def numpy_float_to_device(self, variable):
          variable = torch.from_numpy(variable).float().to(self.device)
          return variable
  
      def load(self, experiences):
          states, actions, new_states, rewards, is_terminals = experiences
          states = torch.from_numpy(states).float().to(self.device)
          actions = torch.from_numpy(actions).long().to(self.device)
          new_states = torch.from_numpy(new_states).float().to(self.device)
          rewards = torch.from_numpy(rewards).float().to(self.device)
          is_terminals = torch.from_numpy(is_terminals).float().to(self.device)
          return states, actions, new_states, rewards, is_terminals
  ```

  

### 持续的更新目标网络

- 先前的方法会将**目标网络**的**权重冻结**多个时间步长，其缺点是
  - 估计所使用的信息(目标网络)是**逐渐陈旧**的。一个更新周期中，大概率越是往后的迭代对于网络训练的提升是无益的。
  - 目标网络的每隔n-steps的更新可能导致网络**过大的改变**，导致损失函数的形状完全被改变。（instability）
  - 这种更新模式在某些层面上可能显得即保守又激进。
- 改变策略，使目标网络的更新不再是冻结，而是减慢。**Polyak Averaging**
  - 每个时间步下，创建一个由**较大比例的目标网络**权重和**较小比例在线网络**权重混合的**目标网络**

$$
\theta_{i}^{-} = \tau\theta_i + (1-\tau) \theta_{i}^{-}
$$

- dueling 网络结构下

$$
\alpha_{i}^{-} = \tau \alpha_i + (1 - \tau) \alpha_{i}^{-}
$$

$$
\beta_{i}^{-} = \tau \beta_i + (1 - \tau) \beta_{i}^{-}
$$

###  Dueling network的特点

- 当有很多**动作的价值相似**时，动作优势函数是**相当的有用的**。
  - 从技术层面看，dueling 的结构**提高了策略估计的能力**，特别是面对许多动作值相似的情况。
  - 使用dueling结构能**更快**，**更准确**的比较(区分)相似动作值。
- 函数逼近(例如神经网络)是有误差的，这是可以预见的。同时因为每个状态动作对是分离的，所以误差对应每个state-action pair是潜在不同的。
- 对于一个动作值函数，状态值函数可以看作是其函数内的一部分（Q =  V + A）。
- 通过dueling 结构，减少了函数(V和Q)的误差和误差的方差。因为Q函数中的**主要误差来源**是V，所以V的稳定带来了Q的稳定。

#### Dueling DDQN的主要过程 

> - 近似动作值函数$Q(s,a;\theta)$
> - 向着最优动作值函数$q^*(s,a)$优化动作值函数。
> - 使用离线策略的TD targets(Q-learning)去估计策略值。$(r+\gamma Q(s',\arg \max \limits_{a'}Q(s',a';\theta_i);\theta^-)$
> - 使用Huber Loss（实际选值为inf,所以实际为MSE）
> - RMSProp，学习速率0.0007。在双网络学习方法中，学习速率偏大一点可能有更好的效果
> - 使用exponentially decaying epsilon-greedy来提高策略。通多大约20,000步，epsilon从1.0衰减到0.3。
> - 回放缓存最小320个样本，最大50,000个样本。mini-batch大小64。
> - 性能评估阶段使用贪婪动作选择策略
> - 不同点：
>   1. 网络结构改变，使用了state-in-values-out结构，但网络内**分为状体和动作评估网络**（节点数为：4，512，128，1；2，2）
>   2. 使用**Polyak averaging策略**。比例为0.9 * target_model + 0.1 * online_model.
> - 不变：
>   1. 采集经验元组$(s_t,A_t,R_{t+1},S_{t+1},D_{t+1})$ 并将经验插入到回放缓存中。
>   2. 随机从缓存中通过均匀分布采样一个mini-batch，为所有mini-batch内的经验计算离线 TD targets(Q-learning)
>   3. 利用MSE和RMSProp拟合动作值函数。

## 优先回放有意义的经验(PER)

### 需要一种更智能的回放经验的方式

- 对于经验回放使用**随机均匀采样**(uniformly random)是**合理但低效的**。因为将训练资源分配给了一些无关经验

- 人类的直观直觉并不一定能很好的决定事务的好坏

  > I love my daughter. I love her so much. In fact, so much that I want her to experience only the
  > good things in life. No, seriously, if you’re a parent, you know what I mean.
  > I noticed she likes chocolate a lot, or as she would say, “a bunch,” so, I started opening up
  > to giving her candies every so often. And then more often than not. But, then she started
  > getting mad at me when I didn’t think she should get a candy.
  > Too much high-reward experiences, you think? You bet! Agents (maybe even humans)
  > need to be reminded often of good and bad experiences alike, but they also need mundane
  > experiences with low-magnitude rewards. In the end, none of these experiences give you the
  > most learning, which is what we’re after. Isn’t that counterintuitive?

### 如何测量经验的“重要性”

- “期望”和“现实”差异较大的经验有着更好的学习意义(优先)
- **TD error的绝对值**表示了这种重要性

$$
|\delta_i| = |r+\gamma Q(s',\arg\max \limits_{a'}Q(s',a';\theta_i,\alpha_i,\beta_i);\theta^-,\alpha^-,\beta^-)-Q(s,a;\theta_i,\alpha_i,\beta_i)|
$$

> **Dueling DDQN target：**$r+\gamma Q(s',\arg\max \limits_{a'}Q(s',a';\theta_i,\alpha_i,\beta_i);\theta^-,\alpha^-,\beta^-)$ 
>
> **Dueling DDQN target error：** $r+\gamma Q(s',\arg\max \limits_{a'}Q(s',a';\theta_i,\alpha_i,\beta_i);\theta^-,\alpha^-,\beta^-)-Q(s,a;\theta_i,\alpha_i,\beta_i)$ 

- absolute TD error 不是表示学习重要性最好的指标，**但是却是最好的替代**。**实际最好的指标是梯度变化的绝对值，但这个做法并不实际**

### 根据TD error“贪婪优先”

假设如下流程：

1. $Env(s,a) = s', r, d(d \to done) $
2. 计算当前状态$s$的动作值$Q(s,a;\theta)$
3. 计算目标值$target = r + \gamma \max \limits_{a'}Q(s',a';\theta)$
4. 计算TD误差的绝对值${absTDerror} = abs(Q(s,a;\theta) - target)$
5. 将经验元组($s,a,r,s',d,absTDerror$)插入回放缓存中。
6. 根据$absTDerror$排序，并取出靠前的经验
7. 根据6取出的经验训练，并循环上述过程。

上述流程中的问题：

- 计算了两次td error：一次在插入回放缓存前，一次在训练网络的时候
- td error的实际值会随着网络的变化而改变，但上述方法不能在每一个时间步下更新所有的TD error，因为效率太低了。

> 一种解决方法：
>
> 只更新回放经验的TD误差，同时将缓存内最大的TD误差赋值给新的经验再插入缓存中，使每个经验都至少能回放一次(因为贪婪）。
>
> 该方法的缺点：
>
> - 在第一次更新中，TD误差为0**的经验永远不可能再被回放**
> - 由于使用函数逼近方法，TD误差收敛的速度是很慢的。这意味着网络的更新会集中在回放缓存的一个小的子集上。最终**TD误差的噪音是很大的**。

- 基于以上问题，基于TD误差的经验采样策略必须是随机的(stochastically)而不是贪婪的(greedily)

### 随机采样优先经验

- 使用随机策略而不是贪婪的原因：
  - 在随机性较大的环境里，贪婪优先策略可能导致学到了噪音
  - 高随机行环境下，单步reward和下一状态的动作值都可能是高随机的。导致TD误差的方差大
  - 非线性函数逼近也导致了TD误差存在噪音，尤其是训练的早期。

### 比例优先策略

$$
p_i = |\delta_i| + \epsilon
$$

$$
P(i) = \frac{p_i^{\alpha}}{\sum_kp^{\alpha}_{k}}, 0\leq \alpha \leq 1
$$

>1. $p_i$ 表示单个经验的有限度，$|\delta_i|$表示TD误差的绝对值，$ \epsilon$ 表示一个常量(为了让TD误差为0的经验依然有采样到的机会)
>
>2. $P(i)$表示采样概率，$\alpha$是控制比例优先的系数
>   - $\alpha = 0$ 是随机均匀采样
>   - $\alpha =1 $,是根据TD误差绝对值采样
>   - $ 0 < \alpha < 1$是上述两种方法的结合

### 基于秩序的优先策略

- **比例优先的缺点在于：对离群数据敏感**
- 改进： 根据回放缓存中的秩序(按照TD误差的绝对值降序)计算采样概率

$$
p_i = \frac{1}{rank(i)}
$$

$$
P(i) = \frac{p_i^{\alpha}}{\sum_kp^{\alpha}_{k}}, 0\leq \alpha \leq 1
$$

### 优先策略的偏差(的修正)

对于几乎所有的off-policy方法都会用到重要性采样（important sampling）。这个技术常常被用来估计一个分布的期望值，而使用其他分布的采样值。对于离策略学习方法，通过目标策略和行为策略产生采样轨迹的概率比值来对回报进行加权。这个概率比称为重要性采样比（importance-sampling ratio）

- **加权重要性采样**(weighted importance sampling)
  - 处理采样数据偏移(bias)
  - 偏移来源：**off-policy的目标策略和行为策略不同**(采样策略)；**非均匀采样**使数据偏移增大了。
- 将加权重要性采样计算方法:


$$
w_i = (NP(i))^{-\beta}
$$

$$
w_i = \frac{w_i}{\max_j(w_j)}
$$

$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s,a,r,s' \sim \mathcal P(\mathcal D)}[\textbf{w}(r+\gamma \max_{a}Q(s',a';\theta^{-})-Q(s,a;\theta_i)) \nabla_{\theta_i}Q(s,a;\theta_I)]
$$



> - $0\leq \beta \leq1 $ 表示矫正的程度，0 为没有矫正的(最后值为1)，1为完全矫正。
> - $\max_j(w_j)$ 归一化加权，使TD误差不会特别大同时保持训练的稳定。
> - 公式(16)中，加权被使用到loss function上，以补偿分布之间的不同。





#### 优先回放有意义的经验的主要过程 

> - 近似动作值函数$Q(s,a;\theta)$
> - 使用了state-in-values-out结构，dueling网络（节点数为：4，512，128，1；2，2）
> - 使用离线策略的TD targets(Q-learning)去估计策略值。$(r+\gamma Q(s',\arg \max \limits_{a'}Q(s',a';\theta_i);\theta^-)$
> - 使用Huber Loss（实际选值为inf,所以实际为MSE）
> - RMSProp，学习速率0.0007。在双网络学习方法中，学习速率偏大一点可能有更好的效果
> - 使用exponentially decaying epsilon-greedy来提高策略。通多大约20,000步，epsilon从1.0衰减到0.3。
> - 回放缓存最小320个样本。mini-batch大小64。
> - 性能评估阶段使用贪婪动作选择策略
> - 使用**Polyak averaging策略**。比例为0.9 * target_model + 0.1 * online_model
> - 不同点：
>   1. 使用加权重要性采样修正TD误差，改变了Loss function
>   2. 使用优先回放缓存(比例优先)，最大10,000, $\alpha = 0.6$，$\beta = 0.1$ 并以0.99992退火。  
> - 主要环节
>   1. 采集经验元组$(s_t,A_t,R_{t+1},S_{t+1},D_{t+1})$ 并将经验插入到回放缓存中。
>   2. 随机从缓存中通过均匀分布采样一个mini-batch，为所有mini-batch内的经验计算**离线** TD targets(Q-learning)，使用double learning.
>   3. 拟合动作值函数$Q(s,a;\theta)$, 利用MSE和RMSProp拟合动作值函数。
>   4. 调整回放缓存中的TD误差